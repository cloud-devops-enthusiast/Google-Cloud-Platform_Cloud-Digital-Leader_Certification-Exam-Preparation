**Storage in GCP**

*Block Storage and File Storage*

- You can say your laptop hard drive as a Block Storage.

- For certain kind of conditions which allows you to share the set of files with multiple users, you can use File Storage.

*Block Storage*

Block storage is more of like same as standard or traditional storage types, this is available in both on-premises and cloud. In this type of storage, the data is stored in blocks. These blocks are managed by the operating system. A single block storage is often connected to a single server, and this is the reason why it is not possible to share the data with multiple users. There is still an option of connecting multiple block storage to a single server, but this is not a recommended approach.

This Block Storage can be used as as Direct Attached Storage (DAS) which allows you for a similar option like of a hard drive. This can also be used as a Storage Area Network (SAN) which is a high speed network connection that allows you to connect a pool of storage devices. This SAN is used by the servers to access the storage devices like Oracle and Microsoft SQL Server.

*File Storage*

File Storage is a type of storage which allows you to share the data with multiple users. It is best suited for workloads like Media Workflows which need huge shared storage for purposes like the video editing or the rendering. This File storage can also be used where there you need to share the files in a secure and organized way among the various virtual servers.

*Block Storage and File Storage in GCP*

- Block Storage in GCP
  - Persistent Disk: Network Block Storage for VMs. Whenever you create a VM, you have attached a persistent disk by default. This persistent disk is a network block storage which is used by the VMs. 
    - Zonal Persistent Disk: Data is replicated within a zone
    - Regional Persistent Disk: Data is replicated within a region or multiple zones
  - Local SSD: Local Block Storage
- File Storage in GCP
  - Cloud Filestore: Managed high performance file storage

*Cloud Storage*

This is the most popular service of GCP which is very scalable as well as very flexible and inexpensive storage service. This service is based on serverless architecture which makes it infinite to scale up and down. This service lets you to store the large objects using a key-value approach. This service treats the whole objects as a single unit and it does not allow you to make any changes to the object. If you want to make any changes to the object, you need to upload the whole object again. This service also gives you the power of access control at the object level which means you can control who can access the object and who cannot. That's why it is known as Object Type Storage.

This service also provides you the option of getting REST API service to access and modify the objects in the bucket. Here you can make use of the CLI (gsutil) to access the objects in the bucket and you can make also use of client libraries like Java, Python, Node.js, etc. to access and modify the objects in the bucket.

Cloud Storage allows you to store all kind of files like text, binary, backup and archives. This also helps in staging the data during the on premise to cloud database migration. This service also provides you the option of storing the data in a multi-regional location or a regional location or a nearline location or a coldline location. This service also provides you the option of storing the data in a multi-regional location or a regional location or a nearline location or a coldline location.

You will always be having low latency access to the data which is stored in the Cloud Storage with the unlimited storage capacity. This service gives you helping hand with autoscaling and no minimum object size. There will be same APIs across the storage classes. For this service to the committed SLA is 99.95% for the multiple region and 99.9% for Standard, Nearline and Coldline Storage classes but there is no committed SLA for the Archive Storage class.

*Storage Classes in Cloud Storage*

<table>
<tr>
<th>Storage Class</th>
<th>Name</th>
<th>Minimum Storage Duration</th>
<th>Typical Monthly Availability</th>
<th>Use Case</th>
</tr>
<tr>
<td>Standard</td>
<td>STANDARD</td>
<td>None</td>
<td>>99.95% (Higher) in multi-region and dual region, 99.9% in region</td>
<td>Frequently used data/Short period of time</td>
</tr>
<tr>
<td>Nearline Storage</td>
<td>NEARLINE</td>
<td>30 Days</td>
<td>99.95% in multi-region and dual region, 99.0% in region</td>
<td>Read or modify once a month or less on average</td>
</tr>
<tr>
<td>Coldline Storage</td>
<td>COLDLINE</td>
<td>90 Days</td>
<td>99.9% in multi-region and dual region, 99.0% in region</td>
<td>Read or modify once a quarter or less on average</td>
</tr>
<tr>
<td>Archive Storage</td>
<td>ARCHIVE</td>
<td>365 Days</td>
<td>99.9% in multi-region and dual region, 99.0% in region</td>
<td>Less than once a year</td>
</tr>
</table>

User can configure the default storage class at a Bucket Level, you can also configure the storage class at an object level. If you add any object without the storage class, then the default storage class will be applied to that object.

*Creating a Persistent Disk*

- Go to search bar and search for Cloud Storage and click on Cloud Storage.

- Click on Create Bucket.

- Give a name to the bucket like here we are giving the name same as the project name. This is because the bucket name should be unique across the GCP.
![Cloud Storage 1](https://github.com/cloud-devops-enthusiast/Google-Cloud-Platform_Cloud-Digital-Leader_Certification-Exam-Preparation/blob/4587c8d986dd0a2314a3e5b0739754b7419ea280/Images/Screenshot%202023-10-29%20205438.png)

- Now moving to the another step of choosing the location where you want to store your data. Here we will be choosing Region as the location type and the region as the us-central1. This is because we want to store our data in a single region.
![Cloud Storage 2](https://github.com/cloud-devops-enthusiast/Google-Cloud-Platform_Cloud-Digital-Leader_Certification-Exam-Preparation/blob/4587c8d986dd0a2314a3e5b0739754b7419ea280/Images/Screenshot%202023-10-29%20210521.png)

- Now moving to the next step of choosing the default storage class. Here we will be choosing the Standard Storage class as the default storage class. This is because we want to store our data in the Standard Storage class. You can also choose the Autoclass which will automatically choose the storage class based on the usage pattern.
![Cloud Storage 3](https://github.com/cloud-devops-enthusiast/Google-Cloud-Platform_Cloud-Digital-Leader_Certification-Exam-Preparation/blob/4587c8d986dd0a2314a3e5b0739754b7419ea280/Images/Screenshot%202023-10-29%20210909.png)

- Moving to the another step of choosing the access control on your objects. Here we will be choosing the Fine Grained option as the access control. This is because we want to have the access control on the object level.
![Cloud Storage 4](https://github.com/cloud-devops-enthusiast/Google-Cloud-Platform_Cloud-Digital-Leader_Certification-Exam-Preparation/blob/4587c8d986dd0a2314a3e5b0739754b7419ea280/Images/Screenshot%202023-10-29%20210938.png)

- In this next step you have option of choosing the protection of your data. Here we will be moving with NONE as we are not interested in any kind of protection.
![Cloud Storage 5](https://github.com/cloud-devops-enthusiast/Google-Cloud-Platform_Cloud-Digital-Leader_Certification-Exam-Preparation/blob/4587c8d986dd0a2314a3e5b0739754b7419ea280/Images/Screenshot%202023-10-29%20210959.png)

- Click on *Create8 and wait till the bucket is created. It will look like this after the bucket is created.
![Cloud Storage 6](https://github.com/cloud-devops-enthusiast/Google-Cloud-Platform_Cloud-Digital-Leader_Certification-Exam-Preparation/blob/4587c8d986dd0a2314a3e5b0739754b7419ea280/Images/Screenshot%202023-10-29%20211602.png)

- Now click on the bucket and click on Upload Files. Upload any file you want to upload.
![Cloud Storage 7](https://github.com/cloud-devops-enthusiast/Google-Cloud-Platform_Cloud-Digital-Leader_Certification-Exam-Preparation/blob/4587c8d986dd0a2314a3e5b0739754b7419ea280/Images/Screenshot%202023-10-29%20211737.png)

- Now after uploading the file click on the file and you will be seeing the Authenticated URL. This URL can be used to access the file.
![Cloud Storage 8](https://github.com/cloud-devops-enthusiast/Google-Cloud-Platform_Cloud-Digital-Leader_Certification-Exam-Preparation/blob/4587c8d986dd0a2314a3e5b0739754b7419ea280/Images/Screenshot%202023-10-29%20211950.png)

- Click on the URL which will open the file in the browser.
![Cloud Storage 9](https://github.com/cloud-devops-enthusiast/Google-Cloud-Platform_Cloud-Digital-Leader_Certification-Exam-Preparation/blob/4587c8d986dd0a2314a3e5b0739754b7419ea280/Images/Screenshot%202023-10-29%20212018.png)

*Cloud Storage Lifecycle Management*

When you create a storage, you can put your files and data in it and the usage of this data will be gradually reduced as the data gets refined and the data gets older. So here comes the need of an automatic switch between different storage classes, here you can make use of the *Object Lifecycle Management* feature of the Cloud Storage.

This whole process of managing the objects happens based on the Age, created time, IsLive, MatchStorageClass, NumberOfNewerVersions etc and can set multiple conditions which can be used to manage the objects in the bucket and should be satisfied to move the objects to a different storage class.

Here are the two types of actions which can be performed on the objects in the bucket.

- SetStorageClass: This action is used to move the objects to a different storage class.

- Deletion: This action is used to delete the objects in the bucket.

*Object Lifecycle Management Examples*

*Deleting objects older than 365 days in a bucket.*

The following configuration document defines a rule to delete all objects older than 365 days in a bucket. This lifecycle configuration is the same as defining a Time to Live (TTL) for each object regardless of when it was added to the bucket.

```
{
  "lifecycle": {
    "rule":
    [
      {
        "action": {"type": "Delete"},
        "condition": {"age": 365}
      }
    ]
  }
}
```

*Deleting live objects after 30 days, and keeping only the 3 most recent versions of each object in a bucket with versioning enabled.*

The following XML document defines a rule to delete any live objects older than 30 days, and another rule to delete archived objects that have at least 3 newer versions (including the live version), which is equivalent to keeping only the 3 most recent versions.

```
{
 "lifecycle": {
  "rule": [
   {
    "action": {
     "type": "Delete"
    },
    "condition": {
     "age": 30,
     "isLive": true
    }
   },
   {
    "action": {
     "type": "Delete"
    },
    "condition": {
     "isLive": false,
     "numNewerVersions": 3
    }
   }
  ]
 }
}
```

This rule moves the objects stored inside the specified bucket to the Nearline Storage class 365 days after the objects are created:

```
{
  "lifecycle": {
    "rule": [
      {
        "action": {
          "type": "SetStorageClass",
          "storageClass": "NEARLINE"
        },
        "condition": {
          "age": 365,
          "matchesStorageClass": [
            "MULTI_REGIONAL",
            "STANDARD",
            "DURABLE_REDUCED_AVAILABILITY"
          ]
        }
      }
    ]
  }
}
```

**Shifting your data to Cloud Storage**

There are two options to shift your data to Cloud Storage.

- Online Transfer: This option makes the use of the gsutil or API to transfer the data to the google cloud storage. This option is best suited for the data which is less than 1 TB and one time transfer. This option is also best suited for the data which is in the on-premises storage or in the other cloud storage.

- Storage Transfer Service: This option is best suited for the data which is more than 1 TB or large scale (petabytes) of online data transfer from the on-premises storage or from any other cloud storage provider liek AWS, Azure or GCP. Here you aslo have a repeatable transfer option which allows you to transfer the data on a regular basis. This also supports the Incremental Transfers which lets you to transfer only the changed data. This is Reliable and Fault Tolerant at the same time, which makes the data transfer process very smooth and seamless. This also lets the user to continue fron where the transfer was left off in case of any failure.

**Transfer Appliance**

Transfer appliance lets you to quickly and securely transfer large and hefty amount of data to the Google Cloud Platform. This appliance is recommended for the data which is more than 20 TB or the online data transfer exceeds the duration of more than 7 days or 1 week. Below is the chart which helps you to choose between the online transfer and the transfer appliance based on the data size and speed of your connection with respect to the time taken to transfer the data.

![Transfer Appliance](https://github.com/cloud-devops-enthusiast/Google-Cloud-Platform_Cloud-Digital-Leader_Certification-Exam-Preparation/blob/77e3b0aa83fef2ecbba21b2f61aff5ca779e6c09/Images/googleSneakernet-f1.jpg)

The process of getting an transfer appliance is simple and easy. You just need to order the transfer appliance from the GCP console and then you will be getting the transfer appliance from the Google. After you receive the transfer appliance, you need to connect it to your network and then you need to transfer the data to the transfer appliance. After the data is transferred to the transfer appliance, you need to ship the transfer appliance back to the Google. After the transfer appliance is received by the Google, the data will be transferred to the Google Cloud Storage.

These appliances provides the faster data transfers with the help of the high speed upto 40Gbps. These appliances are also very secure as they are encrypted with the AES-256 encryption. These appliances are also very reliable as they are fault tolerant and they also provide the option of resuming the transfer from where it was left off in case of any failure. These appliances are also very easy to use as they are very simple to setup and they also provide the option of monitoring the transfer process. You can also order multiple appliances based on the data size and the speed of your network.

Appliances comes mostly in 3 typical sizes

- TA7 

- Rackable TA40 and Freestanding TA40

- Rackable TA300 and Freestanding TA300

*Some Things to Remember:*

- We are using a standard storage class to store all the files in the Cloud Storage Bucket. The files would be frequently accessed for the first 5 days and after that, the files will not be accessed at all. For retaining this compliance in place you can configure the Life cycle policy on the bucket.

- Cloud Storage is a serverless and auto-scaling service.

- If you want to access the files in the Cloud Storage Bucket, Once at the duration of 3 months, then you can use the Coldline Storage Class.

- Persistent Disks offers the Persistent Block Storage in Google Cloud Platform.

- When you plan to move the 100 TB of data to the Google Cloud Platform from your on-premises data center, then you can use the Transfer Appliance.