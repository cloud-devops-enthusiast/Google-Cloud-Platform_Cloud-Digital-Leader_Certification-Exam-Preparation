**Google Cloud Architecture**

*Loose Coupling with Pub/Sub*

Whenever you want to decouple a publisher, consider Pub/Sub. Here Pub/Sub is used in the microservice architectures, IOT Architectures, Streaming Architectures. Here the architecture goes like you have mobile apps, on-prem services running or IOT services which are running and sending stream to Topic which is further being used with the subscriptions which takes the data to their different destinations like datawarehouse or much more.

*Cloud Dataproc*

Cloud dataproc is a fully managed and highly scalable service for running Apache Spark and Apache Hadoop based workloads. It supports variety of jobs like Spark, PySpark, SparkR, Hive, SparkSQL, Pig, Hadoop. This is mainly used to perform the complex batch processing.

It has multiple cluster modes like Single Node, Standard or Highly Availibility which has 3 masters nodes. It makes use of the virtual machine or regular/preemptible VMs.

Usecase: This is used to move your Hadoop and Spark clusters to the cloud. This lets you perform your machine learning and AI development using the Open Source frameworks.

Alternative to dataproc is BigQuery which can be used to run SQL queries on Petabytes. You can go towards Dataproc if you need more than queries (Example: Complex Batch processing machine learning and AI workloads).

Alternative to dataproc is Dataflow which can be simple pipelines without managing clusters and it is serverless so you do not need to manage the servers by yourself.

*Architecture 1 - Big Data Flow - Batch Ingest*

```
                -->     Cloud Dataproc  --|
                |                         | 
Cloud Storage ---->     Data Prep       ------>   BigQuery
                |                         |
                -->     Cloud Dataflow  --|
```

Here the scenario is something like this you have a lot of data in Cloud Storage and you want to process it till bigquery. Here we will be using ETL also known as Extract, Transform and Load to load the data into Bigquery.

Dataprep which is an another service offering by Google Cloud which is used for intelligent data service for visually exploring, cleaning, and preparing data for analysis, reporting and ML else in simple you can say it is used to clean and prepare the data.

Dataflow can be used to create the data pipelines (and do ETL Transactions), it is recommended if you want to have complex logic in place.

If you have hefty much of data processing work with bigdata workloads you can make use of Dataproc service which makes use of Spark and Hadoop.

As after this once your data is in BigQuery you can make use of the Data Studio which is one of the offering by google cloud which is used to visualize your data in BigQuery. It is mostly used to unlock the power of your data with the interactive dashboards and beautiful reports.

You can also make use of Looker where you can connect your data sources so you can get business intelligence out of it.

*Architecture 2 - Streaming Data*

```
                                          ----->    BigQuery
                                          |
Cloud Pub/Sub   --->    Cloud Dataflow  ---
                                          |
                                          ----->    Cloud BigTable
```

Here you have to handle the streaming data. Here we are having Pub/Sub to receive the messages from the subscriptions on the topic. If you have a bit of more complex data processing you can have the Dataflow in place as it will be able to analyse, aggregate and filter the data.

Now you can load your data into BigQuery or whether in Cloud BigTable.

If you have pre-defined time series analytics, storing the data in Bigtable makes more sense that gives you the ability to perform rapid analysis.

For ad hoc complex analysis, you can go ahead with BigQuery.

*Architecture 3 - IOT*

```
Cloud IOT Core
       |
       |
       V
Cloud Pub/Sub
       |
       |        ----->  Cloud Datastore
       V        |
Cloud Dataflow------->  BigQuery
                |
                ----->  Cloud BigTable
```

IOT Core is a offering of Google cloud intended for IOT based workloads, which is used to manage IOT (Registration, Authenticationb and Authorization operations) for devices. Here it used to Send/Receive messages/Real-time telemetry from/to IOT devices.

As you come to know about the word messages you must have Cloud Pub/Sub in place which is a durable message ingestion service (allows the buffering).

Now you can have Dataflow so you can do the ETL and processing data. Here you can have Cloud Functions as well so they can trigger the alerts

At Last we have data storage and analytics usecase that makes,

the IOT Data available to mobile or webapps you can make use of Datastore or filestore,

if you want to execute the pre-defined time series queries or real time analysis you can make use of Bigtable,

if you want to make more complex or ad hoc analytics/analysis you have BigQuery as your solution.

*Architecture 4 - Data Lakes*

![Architecture 4](https://github.com/cloud-devops-enthusiast/Google-Cloud-Platform_Cloud-Digital-Leader_Certification-Exam-Preparation/blob/cafbcf52ccf296a9a510fcb45bb0ecb72892814b/Images/How-to-build-a-data-lake-on-Google-Cloud-Platform-2.png)

It used for big data solutions that have complex. Here the workloads that can be made by collecting, analyzing, reporting, analytics, machine learning and visualizing huge datasets. Here the usecase is like that it should scale seamlessly and also provide flexibility while saving the costs.

Here comes the solution to all above requirements as *Datalake* which is a single platform with the combination of solutions for data storage, data management and data analytics. Here as per the architecture you can have it for storage which gives you the flexibility for the Storage services where the Cloud Storage comes into the picture which has a Low Cost + Durability + Performance + Flexible Processing.

For the data ingestion usecase you can make the utilization of below services based out of usecase. It is one of main thing from the big data lifecycle.
- Streaming Data: Cloud Pub/Sub + Cloud Dataflow
- Batch: Transfer Service + Transfer Appliance + gsutil

Now here the data is available in the cloud storage, now comes the Data processing in the picture.

For Processing and Analytics usecase:
- You can run the SQL queries using BigQuery or you can use Dataproc which is managed hadoop and spark cluster service. Here you can have the data directly from the cloud storage.
- Data Mining and Exploration: You can clean and transform your raw data with dataprep. You can also have cloud datalab (Which is the data science libraries such as Tensorflow and NumPy) for exploring, more of here you can have you code to do so.

*API Challenges*

In today's world most of the applications are build on REST APIs here you have multiple resources like (/todos,/todos/{id},etc) and can have actions in the same way with the HTTP methods like GET, PUT, POST, DELETE, etc.

Managing REST API is not an easy task as:
- Here you've to take care of the authentication and authorization.
- You've to set the limits (rate limiting, quotas) for your API users.
- You've to be carefull while implementing different versions of your APIs.
- You've to implement monitoring, caching and a lot of other features.

Now we will be looking towards the solutions with the APIs in Google Cloud:
- Apigee API Management: It is a comprehensive API management platform
  - It comes with deployment options across Cloud or on-premise or hybrid environment, so is is more of cross cloud platform.
  - APIGEE is a very complex product as it covers out the entire API life cycle, so you can design, secure, publish, analyze, Monitor and Monitize APIs.
  - It has features that let the developer and partners integrating at the same place.
  - It also supports complex integrations like REST, gRPC, NON-gRPC-REST, integrate with GCP, on-premise or hybrid apps.

Cloud Endpoints these are the basic API for management of Google Cloud Backends. It is little complicated to setup: You need to build a container and deploy to cloud run. This supports REST API and gRPC.

API Gateway, which is newer, simpler API management for Google Cloud Backends about which setup process is simple and supports REST API and gRPC.

*Machine Learning*

Machine learning lets you learn from the examples which are further used to create a model, now this model can be used further to make predictions.

Challengs with machine learning is as with the number of examples needed is more and Availability of skilled personnel is also pretty high. The biggest pain regarding implementing machine learning is MLOps.

*ML in Google Cloud*

Machine learning service in google cloud comes with pre-trained models like,

- Speech-to-text API: This converts speech into text.
- Text-to-Speech API: This lets you convert text to speech.
- Translation API: Translate texts into more than one hundred language.
- Natural Language API: This allows you to derive insights from the unstructured text.
- Cloud Vision API: It is a recommended service for generic usecases like identifying there is a cat in a picture or not or in easy way you can say it helps you to classify images into the predefined categories like classifying images, detecting objects or faces, reading the printed words.

Building Custom models in Google Cloud:

- Auto ML: This is a service that allows you to build custom models with minimum ML expertise and efforts. In this service Auto ML Vision lets you build custom models based on images. For example if you want identify the type of houses this service will help you identify hthem based on the categories like Modern, Retro, Flats, Vilas and so on. There is yet another service named AutoML Video Intelligence, this lets you add labels to your video. This also lets you perform streaming video analysis, object detection and tracking. AutoML tables also lets you build models on the structured data.

- If this AUTO ML doesn't works for you, then you can move to Data Scientists to build complex models with the power of frameworks like tensorflow, Pytorch and psykit learn.

- BigQuery ML: This service lets you build models using the simple queries. This makes use of the data from the bigquery dataset here there is no need of exports .

- Vertex AI: This is a service by google cloud that lets you building and deploying the builded models fater. It is like a single platform for multiple needs with custom tooling within a unified AI platform. This covers the entire lifecycle and makes the whole MLops way easy and effective.

*Faster ML in Google Cloud*

If you have heavier workloads that takes models to train for days or weeks for such scenarios you can go with TPU or Tensor Processing Units. These TPUs are intended and fine tuned to run ML workloads as they gives you 20x-30x faster than the traditional workloads. The supported services in google cloud that supports TPU are Google Compute Engine, Google Kubernetes Engine and AI platform. For ML processing with TPUs there is a special image named custom AI platform deep learning VM Image.

*Data is the Key: ML*

When you think from a prespective of training a model is data, Here you need millions of examples or data points. The data needs to be accurate like it should not be biased or have errors in it as there are so many enterprises that face a lot of challenges while cleaning the data.

*Machine Learning Scenarios*

<table>
<tr>
<th>Scenario</th>
<th>Solution</th>
</tr>
<tr>
<td>Translate from one spoken language to another language</td>
<td>Prebuilt Model- Translation API</td>
</tr>
<tr>
<td>Convert Speech to text</td>
<td>Prebuilt Model- Speech to text API</td>
</tr>
<tr>
<td>Generic Identification of objects in an image</td>
<td>Prebuilt Model- Cloud Vision API</td>
</tr>
<tr>
<td>Identify the type of cloud or machine part based on an image</td>
<td>Auto ML Vision</td>
</tr>
<tr>
<td>Simplify the implementation of MLOps</td>
<td>Vertex AI</td>
</tr>
</table>

*Container Registry and Artifact Registry*

- You create application over there respective stacks and have there seperate DBs.
- You make use docker images for running your microservice based applications.
- You store these docker images in Container Registry and Artifact Registry.
- While making use of the Container Registry:
  - Container Registry makes use of GCS (Google Cloud Storage) to store the images
  - This only supports Container images. This does not supports storing JAR files, WAR files or ZIP files or anything other than this.
  - The best alternative to the Container Registry is Docker Hub.
  - If you store images in container registry the url for images will have something like gcr.io which indicates that it uses the google container registry.
  - Here the permissions are managed by managing the access to GCS buckets.
- While Artifact registry comes into place:
  - It is an upgraded version of artifact registry
  - It can manage both container images as well as non-container artifacts.
  - It supports multiple artifact formats like Container Images, Language packages and OS packages.
  - When you want to use the artifact registry you first need to enable its API.
  - You need to create seperate repository and does not make use of the GCS buckets. Repositories can be both regional or multi-regional.
  - The artifact registry uses the pkg.dev in url.
  - You can control the access by using the Artifact registry roles like Artifact Registry Reader/Writer/Administrator etc.
  - You also have right to configure teh repository specific permissions.

*Application Modernization*

Here you have a monolith application which needs to be transformed to Microservice Architecture and also move to the cloud at the same time.

- In such scenarios you can prefer the managed services.
- Prefer Containers and Container Orchestration as it brings flexibility in the process of building and deploying your application.
- If you have such container based services, you can make use of Google Cloud offering GKE.
- As you're running workloads on kubernets on prem and on cloud you can effictively make use of Anthos which lets you to manage them at the same time.
- Always try to create Stateless, Horizontally Scalable Microservices.
- This modernization also lets you to make use of Automation (DevOps, SRE CI-CD). Services like cloud build makes the work way easy.
- One thing to remember in overall way is that you can make use of step by step approach:
  - Experiment and design your proof of concepts.
  - You can replace the application features with the appropriate microservices in phases.

*Google Cloud Security Offerings*

<table>
<tr>
<th>Service</th>
<th>Description</th>
</tr>
<tr>
<td>KMS(Key Management Service)</td>
<td>This service lets you create and manage cryptographic keys (Both Symmetric and Asymmetric). Also it allows you to control their use in your applications and GCP Services. A cryptographic key is used for encrypting and decrypting the data for producing and verifying the digital signatures. You can use this key in various GCP services by using the customer managed encryption key in the advanced settings. For instance refer to creating a storage bucket you can go to its advanced settings and select the customer managed encryption key(CMEK) and select the one which you want to configure from the dropdown.</td>
</tr>
<tr>
<td>Secret Manager</td>
<td>This is used to manage your database passwords, your API keys securely</td>
</tr>
<tr>
<td>Cloud Data Loss Prevention(Cloud DLP)</td>
<td>It can be used to Discover, Classify and Mask Sensitive Data(like the credit card numbers, SSNs, clear text passwords and google cloud credentials). It can be further used with other services of Google cloud like Cloud Storage, BigQuery and Datastore provides APIs that can be invoked from your applications.</td>
</tr>
<tr>
<td>Cloud Armor</td>
<td>This service lets you protect your production apps (at the run time) from denial of service and common web attacks (OWASP Top 10) like XSS(cross site scripting) and SQL injection.</td>
</tr>
<tr>
<td>Web Security Scanner</td>
<td>It lets you to idenitfy the vulnerabilities by running the security tests. Example: Outdated Library, XSS</td>
</tr>
<tr>
<td>Binary Authorization</td>
<td>This service lets you ensure that only trusted container images are deployed to google Cloud.</td>
</tr>
<tr>
<td>Container Threat Detection</td>
<td>This service lets you detect container runtime attacks for example a container downloads a binary and tries to execute it at that point it tries to identify and check for the need of its execution</td>
</tr>
<tr>
<td>Security Command Center</td>
<td>This feature lets you give a consolidated picture of security in Google Cloud (Security Posture Management) Discover Misconfiguration and Vulnerabilities (Built in Threat detection), Compliance Monitoring (Review and Export compilance reports. Check Compliance of your resources with PCI-DSS 3.2.1, OWASP Top Ten, etc).</td>
</tr>
</table>

*Zero Trust Security Model*

- In the traditional IT security Model the security is implemented at the network perimeter. It works like this if you're under the premises you can access the application.
  - It assumes everything inside of it can be trusted.
- Zero Trust: "No Device or Person should be trusted by default, even if they are already inside an organization network" It is more of like trusting no body whether it is inside or outside of the network.
  - This model lets the user go through a strict identity authentication and authorization throughout the network.
  - In Simple words evry user, device and component is considered untrusted at all the times, regardless of whether they are inside or outside of the organization network.
- Three Key Principles:
  - 