**Loosely Coupled Applications with cloud pub/sub**

*Asynchronous Communication: Decoupled*

Applications usually makes the synchronous calls to the logging service, consider a case when your logging service goes down with this whether your application will go down too or In case if there is too much load on the and there are too much logs being generated will the application will go down too.

```
Web Server --> Topic --> Logging Service --> Database
```

Here the Asynchronous Communication for the decoupled applications comes into place, it is more of like everything is placed in seperate block rather than a single block. Here you're intended to create a topic and have your applications put the log messages on the topic. Logging service picks them up for processing when ready.

Advantages of such kind of architecture is that 

- Decoupling: Here the publisher (Apps) don't care about who is listening.

- Availability: Under which the Publisher (Apps) up even if a subscriber (Logging Service) is down.

- Scalability: Scaling the consumer instances (Logging Service) by increasing there number under the high load.

- Durability: Messages are not losted even if the subscriber (Logging Service) is down.

*Pub/Sub*

Pub/Sub is a highly Reliable, Scalable, Fully-Managed asynchronous messaging service which is mostly the backbone for Highly Available and Higly Scalable Solutions. Here you do not need to worry about scaling as it works on autoscaling to process billions of messages per day. At Pub/Sub your costs stays low as you just pay for what you use. Pub/Sub supports both push and pull message deliveries.

Some of the usecase of the Pub/Sub cab be event ingestion and delivery for streaming analytics pipeline.

*Working of Pub/Sub*

- Publisher: Publisher is the sender of the message. here the publisher sends the messages by making HTTPS requests to pubsub.googleapis.com.

- Subscriber: Subscriber is the receiver of the message. On Pull transaction Subscriber pulls the message when it is ready. Here the subscriber makes HTTPS requests to pubsub.googleapis.com. On Push transaction the message is being sent to subscribers, where the Subscriber provide a web hook endpoint at the time of registration. When a message is received on the topic, A HTTPS POST request is sent to the webhook endpoints.

It is flexible with the subcribers and publishers relationships: It can be One to Many, Many to One and Many to Many.

*Pub/Sub: Getting ready with Topic and Subscriptions*

1. Topic is created.

2. Subscriptions are created.
    - Subcribers register to the topic
    - Each subscription represents discrete pull of messages from a topic
        - Multiple Clients pull the same subcription => Messages splits between the clients
        - Multiple Clients create a subscription each => Each client will get every message

*Pub/Sub: Sending and Receiving the messages*

- Publisher sends the message to a topic.

- Message gets individually delivered to each other and every subscription
    - Subscriber can receive message either by:
        - Push: Pub/Sub sends the message to a subscriber.
        - Pull: Subcriber poll for messages

- Subscriber sends the acknowledgements

- Messages are removed from the subscriptions message queue.
    - Pub/Sub ensures the message is retained per subcription until it is acknowledged.

*Options in Creating Subscription*

- You can set the delivery type as Pull and Push. If you select delivery type as Puh, You're needed to give an endpoint URL.

- You can also set the message retention period, which is regarding retention of the unacknowledged messaged for a duration of time and if you switch this on your messages will be retained for the defined duration.

- You can also delete your subscription automatically if there is no subscriber activity such as open connections, active pull and sucessfull pushes. You can set this up at your convinience in how many days it should expire. Just keep this in mind this period must be longer than the message retention duration.

- You have option to set the acknowledgement deadline that allows the subscriber to acknowledgement receipt before sending the message. By default it is set to 10 seconds.

- You can also set the subscription filter which matches the defined filter.

- You can have dead lettering setted up in case of maximum number of delivery attempts. If a message is not delivered it can be republished to the specific dead letter topic.
 
- For sending the messages manually you can go to message tab and send the message like and can send them likewise and after opening the subscription do the pull and you wil be able to get all the messages.

- Till here you was able to pull the messages but now you have to acknowledge the messages as well so for that enable ACK messages and click on pull and now you can send acknowdlegement you can send likewise.

*Cloud Dataflow*

Dataflow is a service by google cloud that provides unified stream and batch data processing at scale. Dataflow lets you create data pipelines that read data from one or more sources, transform the data and write the data to its destination. Use Case for data flow can be data movement involving ingesting data or replicating data across subsystems. In other words it is used to create pipelines and route the data. Cloud dataflow serverless and 

Few examples of pipelines that you can build:
- Pub/Sub > Dataflow > BigQuery(Streaming)
- Pub/Sub > Dataflow > Cloud Storage(Streaming-files)
- Cloud Storage > Dataflow > BigTable/CloudSpanner/Datastore/BigQuery (Batch - Load data into databases)
- Bulk Compress of files in Cloud Storage(Batch)
- Convert file formats between Avro, Parquet and csv(Batch)

Streaming and batch usecases which involves Realtime Fraud Detection, Sensor data processing, Log data processing, batch processing (Load data, convert formats, etc). You can also use pre-build templates. It is based on Apache Beam (Supports java, Python, Go...)